# import tensorflow as tf
# print(tf.__version__)
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import tensorflow as tf
import numpy as np

#特征工程的作用
'''
数据特征决定了模型的上限
预处理和特征提取是最核心的
算法和参数选择决定了如何逼近这个上限
 
'''
#深度学习解决怎么提取特征
#数据增强
#计算机视觉
#图像分类任务
#一张图片可以看成一个三维的矩阵 数值越大，图像越亮
#部分遮蔽，背景混入
#

'''
机器学习的常规套路
1.收集数据并给定标签
2.训练一个分类器
3.测试，评估

'''
#k近邻算法
'''
1.已知类别数据集的点与当前点的距离
2.按照距离依次排序
3.选取与当前距离最小的k个点
4.确定前k个点所在类别出现概率
5.返回前k个点出现频率最高的类别作为当前预测点


k近邻不适合用来进行分类任务
'''



#损失函数
#可以做分类，也可以做评估
#例如损失函数L=max(0,sj-sy+1)  取0和sj-sy+1中较大的数



#有监督任务，手里有相应的标签，可以知道实际的输出
#正则化惩罚项=v(w1^2+w2^2+......) v为惩罚系数，v越大防止过拟合，神经网络能解决很多问题，但缺点是其功能过于强大

#概率
#sigmoid函数
#将得分值转化为概率值
#exp放大数据的差距即e的^x次方 ，再将其归一化转化为【0,1】的概率值，取对数求它的损失  (一般为softmax,或者sigmoid)


#回归任务，根据它的得分值去计算它的损失
#分类任务，由概率值计算它的损失

#前向传播 ，（反向传播）
#梯度下降，根据它的loss,从后往前求偏导，求出每一层的偏导再继续修改，计算反向传播（梯度下降）链式法则从后往前求偏导
#门单元：均等分配（如x+y,那么反向传播传出的梯度传给x和y都是一样的），MAX门单元如max(0,x)
#乘法门单元：互换

#f为非线性激活函数，每一层都有非线性激活函数
#理论上神经元个数越多，层数越多，效果更好，过拟合程度更大

#正则化 训练集的loss+R(w)  惩罚系数变大，把过拟合调低，要看test集怎么样，训练集再怎么好，也没有什么很好的作用
#激活函数，sigmoid函数，一但数值过大或过小，梯度可能等于0，那么之前的梯度就不会更新
#激活函数，relu函数，y=max(0,x),x<0,值全为0，大于0时，y=x,不会出现梯度为0的情况


#数据预处理
#标准化操作 中心化操作每个xi都去减均值u,然后除以标准差得到处理后的数据

#参数初始化，最开始是给出一组随机值，然后通过反向传播进行修改
#w=0.01*np.random.randn(D,H),比如想生成3*4的权重矩阵，那么D=3,H=4 。（*0.01是把浮动减小）
#抗过拟合方法二，DROP-OUT方法，在每一次训练过程中随机杀死一些神经元（可以选择一定比例），
#输入x12,x2,x3……数据特征，









#支持向量机













